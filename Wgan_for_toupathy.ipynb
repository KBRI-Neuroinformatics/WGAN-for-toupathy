{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from numpy import array\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True \n",
    "if use_gpu :\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('XLA_GPU')))\n",
    "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gene_filtering(genepath):    \n",
    "\n",
    "    # Ensembl ID\n",
    "    gnames = np.array(pd.read_csv(genepath))[:,0]\n",
    "    \n",
    "    # Gene symbol : gene name\n",
    "    glists = np.array(pd.read_csv(genepath))[:,1]\n",
    "\n",
    "    return gnames, glists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filtered_gene(datapath, gnames):\n",
    "    \n",
    "    #Load dataset\n",
    "    rld = np.genfromtxt(datapath, delimiter = ',', dtype = 'str')\n",
    "    label, gene, value = rld[0][1:], rld[1:,0], rld[1:,1:].T\n",
    "    label = np.array([x.replace('\"', '') for x in label])\n",
    "    gene = np.array([x.replace('\"', '') for x in gene])\n",
    "    value = value.astype(np.float32)\n",
    "\n",
    "    #Extract filtered genes\n",
    "    eidx = []\n",
    "    for g in range(len(gnames)):\n",
    "        idx = np.where(gnames[g] == gene)[0][0]\n",
    "        eidx.append(idx)\n",
    "\n",
    "    evalue, egene = value[:, eidx], gnames\n",
    "    return evalue, egene, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(evalue, egene, elabel, save=True):\n",
    "\n",
    "    aug_values = []\n",
    "    aug_labels = []    \n",
    "    sp_size = 5    \n",
    "    n_age = 2\n",
    "    n_treatment = 2\n",
    "        \n",
    "    for ti in range(n_age): #time\n",
    "        \n",
    "        for tr in range(n_treatment): #treatment\n",
    "            #\n",
    "            for s in range(sp_size): #samples\n",
    "                s1 = ti*10+tr*5+s\n",
    "                for s_ in range(s, sp_size):\n",
    "                    s2 = ti*10+tr*5+s_\n",
    "                    tmp1, tmp2 = evalue[s1], evalue[s2]\n",
    "                    if(s==s_): #rawdata\n",
    "                        aug_values.append(tmp1)\n",
    "                        aug_labels.append(elabel[s1])\n",
    "                    else: #augmentation\n",
    "                        for r in range(9):\n",
    "                            augx = tmp1*(r+1)*0.1+tmp2*(9-r)*0.1\n",
    "                            aug_values.append(augx)\n",
    "                            aug_labels.append(elabel[s1]+'+'+elabel[s2]+'+'+str(r+1))\n",
    "                            \n",
    "    aug_values, aug_labels = np.array(aug_values), np.array(aug_labels),\n",
    "\n",
    "    #save\n",
    "    if save==True:\n",
    "        np.savez('augmented_input_Tau_Union_LFC03_3767G_re.npz', genelist=egene, values=aug_values, \n",
    "                 labels=aug_labels)\n",
    "        print ('Augmented input is saved!')\n",
    "    \n",
    "    return [egene, aug_values, aug_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerun Augmentation?\n",
    "re_data_augmentation = True\n",
    "is_training=True\n",
    "is_interpolation = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Gene filtering and augmentation\n",
    "\n",
    "\n",
    "\n",
    "if re_data_augmentation : \n",
    "    gene_path = 'Tau_Union_rld_LFC03_remove_all_case_3767genes.csv'\n",
    "    ex_gnames, ex_glists = gene_filtering(gene_path)\n",
    "    ex_value, ex_gene, ex_label = extract_filtered_gene(gene_path, ex_gnames)\n",
    "    augmented_data = data_augmentation(ex_value, ex_glists, ex_label, True)\n",
    "    input_ = 'augmented_input_Tau_Union_LFC03_3767G_re.npz'\n",
    "else : \n",
    "    input_ = 'augmented_input_Tau_Union_LFC03_3767G.npz'\n",
    "    \n",
    "dat = np.load(input_,allow_pickle=True)\n",
    "rlds, genelist, augsample = dat['values'], dat['genelist'], dat['labels']\n",
    "realidx = [0, 95, 190, 285]\n",
    "\n",
    "print (rlds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Indexing\n",
    "\n",
    "WT3M = np.array([0,37,65,84,94])\n",
    "AD3M = np.add(WT3M, 95)\n",
    "WT6M = np.add(WT3M, 2*95)\n",
    "AD6M = np.add(WT3M, 3*95)\n",
    "Aug_WT3M = np.arange(95)\n",
    "Aug_AD3M = np.add(Aug_WT3M,95)\n",
    "Aug_WT6M = np.add(Aug_WT3M,2*95)\n",
    "Aug_AD6M = np.add(Aug_WT3M,3*95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Rescaling(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_list = [Aug_WT3M, Aug_WT6M, Aug_AD3M, Aug_AD6M]\n",
    "\n",
    "rld_mean = np.average(rlds, axis=0)\n",
    "rld_median = np.median(rlds, axis=0)\n",
    "rld_std = np.zeros((len(cond_list),rlds.shape[1]))\n",
    "\n",
    "for c in range(len(cond_list)):\n",
    "    rld_std[c] = np.std(rlds[cond_list[c]], axis=0)\n",
    "\n",
    "max_rld_std = np.max(rld_std, axis=0)\n",
    "avg_rld_std = np.average(rld_std, axis=0)\n",
    "all_rld_std = np.std(rlds,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IQR filter \n",
    "\n",
    "num_q1, num_q3 = 25, 75 \n",
    "\n",
    "all_q1 = stats.scoreatpercentile(all_rld_std, num_q1)\n",
    "max_q1 = stats.scoreatpercentile(max_rld_std, num_q1)\n",
    "\n",
    "all_q3 = stats.scoreatpercentile(all_rld_std, num_q3)\n",
    "max_q3 = stats.scoreatpercentile(max_rld_std, num_q3)\n",
    "\n",
    "all_rld_std_filter = np.where(all_rld_std<all_q1,all_q1,all_rld_std)\n",
    "all_rld_std_filter_f = np.where(all_rld_std>all_q3,all_q3,all_rld_std_filter)\n",
    "\n",
    "max_rld_std_filter = np.where(max_rld_std<max_q1,max_q1,max_rld_std) \n",
    "max_rld_std_filter_f=np.where(max_rld_std>max_q3,max_q3,max_rld_std_filter) \n",
    "\n",
    "two_rld_std_f = []\n",
    "two_rld_std_f.append(all_rld_std_filter_f)\n",
    "two_rld_std_f.append(max_rld_std_filter_f)\n",
    "two_rld_std_f = np.array(two_rld_std_f)\n",
    "\n",
    "gmean_rld_std_f = stats.gmean(two_rld_std_f,axis=0)\n",
    "\n",
    "re_rld_f = (rlds-rld_mean)/gmean_rld_std_f\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(re_rld_f.flatten(), bins=1000)\n",
    "plt.xlim(-5, 5)\n",
    "plt.plot()  \n",
    "\n",
    "std_re_rld_f = np.std(re_rld_f.flatten())\n",
    "\n",
    "### Within 95% \n",
    "re_rld_norm_f = re_rld_f/(2*1.959*std_re_rld_f)+0.5\n",
    "\n",
    "rld = re_rld_norm_f\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(re_rld_norm_f.flatten(), bins=1000)\n",
    "plt.xlim(-0.1, 1.1)\n",
    "plt.plot()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training networks setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training and test dataset\n",
    "\n",
    "nrld = rld.flatten()\n",
    "te_ratio = 0.1\n",
    "\n",
    "x_idx = np.arange(len(rld))\n",
    "te_idx = np.random.choice(len(rld), int(len(rld)*te_ratio), False)\n",
    "tr_idx = np.setdiff1d(x_idx, te_idx)\n",
    "xtr, xte = rld[tr_idx], rld[te_idx]\n",
    "tr_max = np.max(xtr)\n",
    "n_tr, n_te = len(xtr), len(xte)\n",
    "\n",
    "print ('xtr:', xtr.shape, ', xte:', xte.shape)\n",
    "n, p = xtr.shape\n",
    "print(n ,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter setting\n",
    "\n",
    "lr = 1e-5                        # Learning rate\n",
    "EPOCHS = 300000                  # Epochs to train\n",
    "critic = 5                       # Critic updates per generator update \n",
    "gradient_penalty_weight = 10.0\n",
    "BUFFER_SIZE = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Number of generator hidden layer units & Number of discriminator hidden layer units\n",
    "gen_size, disc_size = 450, 270\n",
    "rld = rld.astype('float32')\n",
    "rld_max = np.max(rld)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(rld).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer setting\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the training data directory\n",
    "\n",
    "log_dir=\"training_data/\"\n",
    "sub_path = log_dir + \"Tau_union_LFC03_G3767_G450D270_300k/\" + \"20210309_1\"\n",
    "\n",
    "if is_training :\n",
    "    # Set the directory to save the training data \n",
    "    sub_path = log_dir + \"Tau_union_LFC03_G3767_G450D270_300k/\" + \"yours_dir\"\n",
    "    summary_writer = tf.summary.create_file_writer(sub_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100\n",
    "nrld = rld.flatten()\n",
    "\n",
    "def get_noise(batch_size):\n",
    "    seed = nrld[np.random.randint(0,len(nrld),size=(batch_size, noise_dim))]\n",
    "    seed = tf.convert_to_tensor(seed)\n",
    "    return seed\n",
    "\n",
    "idx_ = np.random.choice(n_tr, BATCH_SIZE, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_init=0.3\n",
    "def make_generator_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(gen_size, input_shape=(noise_dim, ), \n",
    "                     kernel_initializer=tf.random_uniform_initializer(-gen_init,gen_init)),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dense(gen_size, kernel_initializer=tf.random_uniform_initializer(-gen_init,gen_init)),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dense(p, kernel_initializer=tf.random_uniform_initializer(-gen_init,gen_init))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = get_noise(1)\n",
    "generated_rlds = generator(noise, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(disc_size, input_shape=(p, )),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dense(disc_size),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dense(1)\n",
    "    ])   \n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = discriminator(generated_rlds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = sub_path + \"/training_checkpoints\"\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_gen(rlds, noise):\n",
    "    #batch_size = rlds.get_shape().as_list()[0]\n",
    "    \n",
    "    with tf.GradientTape() as t:\n",
    "        #noise = get_noise(batch_size)\n",
    "        generated_rlds = generator(noise, training=True)\n",
    "        fake_output = discriminator(generated_rlds, training=True)\n",
    "        \n",
    "        loss = generator_loss(fake_output)\n",
    "    gradients = t.gradient(loss, generator.trainable_variables)\n",
    "   \n",
    "    generator_optimizer.apply_gradients(zip(gradients, generator.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_disc(rlds, noise):\n",
    "    #batch_size = rlds.get_shape().as_list()[0]\n",
    "    \n",
    "        \n",
    "    with tf.GradientTape() as t:\n",
    "        \n",
    "        generated_rlds = generator(noise, training=True)\n",
    "        fake_output = discriminator(generated_rlds, training=True)\n",
    "        real_output = discriminator(rlds, training=True)\n",
    "        \n",
    "        gp = gradient_penalty(rlds, generated_rlds)\n",
    "        loss = discriminator_loss(fake_output, real_output) + gp\n",
    "        gradients = t.gradient(loss, discriminator.trainable_variables)\n",
    "           \n",
    "    discriminator_optimizer.apply_gradients(zip(gradients, discriminator.trainable_variables))\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    fake_loss = -tf.reduce_mean(fake_output)\n",
    "    return fake_loss\n",
    "\n",
    "def discriminator_loss(fake_output, real_output):\n",
    "    fake_loss = tf.reduce_mean(fake_output)\n",
    "    real_loss = tf.reduce_mean(real_output)\n",
    "    return fake_loss - real_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient penalty \n",
    "def gradient_penalty(rlds, generated_rlds):\n",
    "    batch_size = rlds.get_shape().as_list()[0]\n",
    "    epsilon = tf.random.uniform (shape=[batch_size, 1], minval=0., maxval=rld_max)\n",
    "    x_hat = (epsilon * rlds) + ((1 - epsilon) * generated_rlds)\n",
    "    #x_hat = (epsilon * generated_rlds) + ((1 - epsilon) * rlds)\n",
    "    with tf.GradientTape() as gp:\n",
    "        gp.watch(x_hat)\n",
    "        d_hat = discriminator(x_hat, training=True)\n",
    "    gradients = gp.gradient(d_hat, [x_hat])[0]\n",
    "    d_gradient = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=1))\n",
    "    d_regularizer = gradient_penalty_weight * tf.reduce_mean(tf.square(d_gradient-1.))\n",
    "    return d_regularizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_training :\n",
    "    gen_loss_results = []\n",
    "    disc_loss_results = []\n",
    "    gen_rlds = []\n",
    "\n",
    "    start = time.time()\n",
    "    num_to_generate = 84\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_gen_loss_avg = tf.keras.metrics.Mean()\n",
    "        epoch_disc_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "        for batch in train_dataset:\n",
    "            noise = get_noise(batch.shape[0])\n",
    "            for _ in range (critic):\n",
    "                #train_disc(batch)\n",
    "                d_loss = train_disc(batch, noise)           \n",
    "                epoch_disc_loss_avg(d_loss)\n",
    "            g_loss = train_gen(batch, noise)\n",
    "            epoch_gen_loss_avg(g_loss)\n",
    "            #train_gen(batch)\n",
    "\n",
    "        gen_loss_results.append(epoch_gen_loss_avg.result())\n",
    "        disc_loss_results.append(epoch_disc_loss_avg.result())\n",
    "\n",
    "        if (epoch + 1) % 500 == 0:\n",
    "            checkpoint_prefix = checkpoint_dir + \"/cp-\" + str(epoch+1) + '.ckpt'\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix) \n",
    "            print(\"Saving checkpoint for epoch{} at {}\".format(epoch+1, checkpoint_prefix))\n",
    "\n",
    "            predictions = generator(get_noise(num_to_generate), training=False)\n",
    "            gen_rlds.append(predictions.numpy())\n",
    "            print(\"Epoch {:03d}: Gen_Loss: {:.3f}, Disc_Loss: {:.3f}\". format(epoch+1, \n",
    "                                                                      epoch_gen_loss_avg.result(),\n",
    "                                                                      epoch_disc_loss_avg.result()))\n",
    "\n",
    "        #epoch_gen_loss_avg.reset_states()\n",
    "        #epoch_disc_loss_avg.reset_states()\n",
    "\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('gen_loss', epoch_gen_loss_avg.result(), step=epoch)\n",
    "            tf.summary.scalar('disc_loss', epoch_disc_loss_avg.result(), step=epoch)\n",
    "\n",
    "    elapsed_time = time.time() - start\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print (\"Elapsed time: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "\n",
    "    predictions = generator(get_noise(num_to_generate), training=False)\n",
    "    gen_rlds.append(predictions.numpy())\n",
    "    gen_x_84 = np.array(gen_rlds)\n",
    "    np.savez(sub_path+'/Loss_GenX84.npz', gen_x_84=gen_x_84,disc_loss=disc_loss_results,gen_loss=gen_loss_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reload_LossGenX = np.load(sub_path+'/Loss_GenX84.npz')\n",
    "Rel_gen_x_84 = Reload_LossGenX['gen_x_84'] \n",
    "Rel_disc_loss = Reload_LossGenX['disc_loss']\n",
    "Rel_gen_loss = Reload_LossGenX['gen_loss']\n",
    "print(Rel_gen_x_84.shape, np.shape(Rel_disc_loss), np.shape(Rel_gen_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_x, disc_loss_results, gen_loss_results = Rel_gen_x_84, Rel_disc_loss, Rel_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss graph\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(disc_loss_results, c='blue', label='discriminator')\n",
    "plt.plot(gen_loss_results, c='red', label='generator')\n",
    "plt.legend(loc=1, fontsize=14, bbox_to_anchor=(1.01,1.01))\n",
    "plt.ylim(-6.0,10.0)\n",
    "plt.yticks([-6,-2,0,2,6,10], ['-6', '-2', '0', '2', '6', '10'], fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.xticks(np.arange(0, 350000, 50000), ['0', '50K','100K','150K', '200K', '250K', '300K'], \n",
    "           fontsize=16)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tSNE\n",
    "initx = np.concatenate((rld, gen_x[100]), axis=0)\n",
    "med1x = np.concatenate((rld, gen_x[200]), axis=0) #100K\n",
    "med2x = np.concatenate((rld, gen_x[400]), axis=0) #200K\n",
    "endx = np.concatenate((rld, gen_x[600]), axis=0)  #300K\n",
    "print (initx.shape, med1x.shape, med2x.shape, endx.shape)\n",
    "\n",
    "tsne = TSNE()\n",
    "tsne_init = tsne.fit_transform(initx)\n",
    "tsne_med1 = tsne.fit_transform(med1x)\n",
    "tsne_med2 = tsne.fit_transform(med2x)\n",
    "tsne_end = tsne.fit_transform(endx)\n",
    "print (tsne_init.shape, tsne_med1.shape, tsne_med2.shape, tsne_end.shape)\n",
    "print (len(rld), len(gen_x[0]))\n",
    "n_rld = len(rld)\n",
    "n_gen = len(gen_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,3))\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.title(\"50K epoch\")\n",
    "plt.scatter(tsne_init[:n_rld,0], tsne_init[:n_rld,1], alpha=0.5, c='red')\n",
    "plt.scatter(tsne_init[n_rld:n_rld+n_gen,0], tsne_init[n_rld:n_rld+n_gen,1], alpha=0.5, c='blue')\n",
    "plt.scatter(tsne_init[WT3M,0], tsne_init[WT3M,1], alpha=0.7, c='gold')\n",
    "plt.scatter(tsne_init[AD3M,0], tsne_init[AD3M,1], alpha=0.7, c='gold')\n",
    "plt.scatter(tsne_init[WT6M,0], tsne_init[WT6M,1], alpha=0.7, c='gold')\n",
    "plt.scatter(tsne_init[AD6M,0], tsne_init[AD6M,1], alpha=0.7, c='gold')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.title(\"100K epoch\")\n",
    "plt.scatter(tsne_med1[:n_rld,0], tsne_med1[:n_rld,1], alpha=0.5, c='red')\n",
    "plt.scatter(tsne_med1[n_rld:n_rld+n_gen,0], tsne_med1[n_rld:n_rld+n_gen,1], alpha=0.5, c='blue')\n",
    "plt.scatter(tsne_med1[WT3M,0], tsne_med1[WT3M,1], alpha=0.7, c='gold')\n",
    "plt.scatter(tsne_med1[AD3M,0], tsne_med1[AD3M,1], alpha=0.7, c='gold')\n",
    "plt.scatter(tsne_med1[WT6M,0], tsne_med1[WT6M,1], alpha=0.7, c='gold')\n",
    "plt.scatter(tsne_med1[AD6M,0], tsne_med1[AD6M,1], alpha=0.7, c='gold')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.title(\"200K epoch\")\n",
    "plt.scatter(tsne_med2[:n_rld,0], tsne_med2[:n_rld,1], alpha=0.5, c='red')\n",
    "plt.scatter(tsne_med2[n_rld:n_rld+n_gen,0], tsne_med2[n_rld:n_rld+n_gen,1], alpha=0.5, c='blue')\n",
    "plt.scatter(tsne_med2[WT3M,0], tsne_med2[WT3M,1], alpha=0.7, c='gold')\n",
    "plt.scatter(tsne_med2[AD3M,0], tsne_med2[AD3M,1], alpha=0.7, c='gold')\n",
    "plt.scatter(tsne_med2[WT6M,0], tsne_med2[WT6M,1], alpha=0.7, c='gold')\n",
    "plt.scatter(tsne_med2[AD6M,0], tsne_med2[AD6M,1], alpha=0.7, c='gold')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.title(\"300K epoch\")\n",
    "plt.scatter(tsne_end[:n_rld,0], tsne_end[:n_rld,1], alpha=0.5, c='red')\n",
    "plt.scatter(tsne_end[n_rld:n_rld+n_gen,0], tsne_end[n_rld:n_rld+n_gen,1], alpha=0.5, c='blue')\n",
    "plt.scatter(tsne_end[WT3M,0], tsne_end[WT3M,1], alpha=0.7, c='gold')\n",
    "plt.scatter(tsne_end[AD3M,0], tsne_end[AD3M,1], alpha=0.7, c='gold')\n",
    "plt.scatter(tsne_end[WT6M,0], tsne_end[WT6M,1], alpha=0.7, c='gold')\n",
    "plt.scatter(tsne_end[AD6M,0], tsne_end[AD6M,1], alpha=0.7, c='gold')\n",
    "plt.legend(['Training','Generated', 'Original Real'], ncol=3, bbox_to_anchor=(-.22, -.2), fontsize=14)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_noise = 10000\n",
    "  \n",
    "Fake10000z = get_noise(n_noise)\n",
    "Fake10000 = generator(Fake10000z, training=False)\n",
    "print(generator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if is_training :\n",
    "#generate augx for all samples\n",
    "#caluclate Corr(fake_x, xte)\n",
    "fake_real_corr = np.zeros((n_rld,))\n",
    "Fakeallz, Fakeallx = np.zeros((n_rld,Fake10000z.shape[1])), np.zeros((n_rld,Fake10000.shape[1]))\n",
    "print (fake_real_corr.shape)\n",
    "\n",
    "genaugx = []\n",
    "for i in range(n_rld):\n",
    "    scorr_ary = np.zeros((n_noise,))\n",
    "    tmp1 = rld[i]\n",
    "    for j in range(n_noise):\n",
    "        tmp2 = Fake10000[j]\n",
    "        scorr_ary[j] = stats.pearsonr(tmp1, tmp2)[0]\n",
    "    smaxidx = np.argsort(scorr_ary)[-10:]\n",
    "\n",
    "    Fakeallz[i] = np.average(tf.gather(Fake10000z, smaxidx), axis=0)\n",
    "    Fakeallx[i] = generator(np.reshape(Fakeallz[i], (1, 100)), training=False)\n",
    "    fake_real_corr[i] = stats.pearsonr(tmp1, Fakeallx[i])[0]\n",
    "    genaugx.append(Fakeallx[i])\n",
    "genaugx = np.array(genaugx)\n",
    "genaugx = genaugx.reshape((rld.shape))\n",
    "print(genaugx.shape)\n",
    "#save value\n",
    "np.savez(sub_path+'/FakeAll_atEnd.npz', fake_real_corr=fake_real_corr,Fakeallx=Fakeallx,genaugx=genaugx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reload_FakeA_atE = np.load(sub_path+'/FakeAll_atEnd.npz')\n",
    "Rel_fake_real_corr, Rel_Fakeallx, Rel_genaugx = Reload_FakeA_atE['fake_real_corr'],Reload_FakeA_atE['Fakeallx'],Reload_FakeA_atE['genaugx']\n",
    "print(Rel_fake_real_corr.shape, np.shape(Rel_Fakeallx), np.shape(Rel_genaugx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_real_corr, Fakeallx, genaugx = Rel_fake_real_corr, Rel_Fakeallx, Rel_genaugx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(fake_real_corr,axis=0))\n",
    "real_min = np.argmin(fake_real_corr)\n",
    "plt.scatter(Fakeallx[real_min],rld[real_min])\n",
    "plt.plot([0,1],color='red')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(fake_real_corr,axis=0))\n",
    "real_max = np.argmax(fake_real_corr)\n",
    "plt.scatter(Fakeallx[real_max],rld[real_max])\n",
    "plt.plot([0,1],color='red')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_re_rld(real, fake, bins):\n",
    "    plt.figure(figsize=(20,7))\n",
    "    plt.suptitle('Histogram of rescaled RLD', fontsize=36, y=1.03)\n",
    "    plt.subplot(121)\n",
    "    plt.hist(real.flatten(), bins=bins)\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    plt.ylim(0, 10000)\n",
    "    plt.xticks(fontsize=34)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Rescaled RLD', fontsize=35)\n",
    "    plt.ylabel('Counts', fontsize=35)\n",
    "    plt.title('Real', fontsize=35)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.subplots_adjust(wspace=0.05)\n",
    "    plt.hist(fake.flatten(), bins=bins, color='red')\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    plt.ylim(0, 10000)\n",
    "    plt.xticks(fontsize=34)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Rescaled RLD', fontsize=35)\n",
    "    plt.title('Generated', fontsize=35)\n",
    "    publish_save_dir = 'publish_data/'\n",
    "    file_name = 'histogram_RLD.jpeg'\n",
    "\n",
    "    plt.savefig(publish_save_dir+file_name, bbox_inches='tight',  pad_inches=0, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numBin = int((np.max(rld.flatten())-np.min(rld.flatten()))/0.005)\n",
    "plot_hist_re_rld(rld, genaugx, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate performance of network by comparing distribution of real and fake\n",
    "\n",
    "real_corr = np.zeros((rld.shape[0], rld.shape[0]))\n",
    "for i in range(rld.shape[0]):\n",
    "    for j in range(rld.shape[0]):\n",
    "        tmp1, tmp2 = rld[i], rld[j]\n",
    "        real_corr[i,j] = stats.pearsonr(tmp1, tmp2)[0]\n",
    "\n",
    "aug_corr = np.zeros((rld.shape[0], rld.shape[0]))\n",
    "for i in range(len(aug_corr)):\n",
    "    for j in range(len(aug_corr)):\n",
    "        tmp1, tmp2 = rld[i], genaugx[j]\n",
    "        aug_corr[i,j] = stats.pearsonr(tmp1,tmp2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_corr(real, fake, bins=100):\n",
    "    plt.figure(figsize=(20,7))\n",
    "    plt.suptitle('Histogram of correlation coefficient', fontsize=36, y=1.03)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(real.flatten(), bins=bins)\n",
    "    plt.xlim(-1,1)\n",
    "    plt.xticks([-1, -0.5, 0.0, 0.5,  1], fontsize=24)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Correlation', fontsize=35)\n",
    "    plt.ylabel('Counts', fontsize=35)\n",
    "    plt.title('Real vs. Real', fontsize=35)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.subplots_adjust(wspace=0.12)\n",
    "    plt.hist(fake.flatten(), bins=bins, color='red')\n",
    "    plt.xlim(-1,1)\n",
    "    plt.xticks([-1, -0.5, 0.0, 0.5,  1], fontsize=24)\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Correlation', fontsize=35)\n",
    "    plt.title('Real vs. Generated', fontsize=35)\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist_corr(real_corr, aug_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Indexing\n",
    "\n",
    "WT3M = np.array([0,37,65,84,94])\n",
    "AD3M = np.add(WT3M, 95)\n",
    "WT6M = np.add(WT3M, 2*95)\n",
    "AD6M = np.add(WT3M, 3*95)\n",
    "Aug_WT3M = np.arange(95)\n",
    "Aug_AD3M = np.add(Aug_WT3M,95)\n",
    "Aug_WT6M = np.add(Aug_WT3M,2*95)\n",
    "Aug_AD6M = np.add(Aug_WT3M,3*95)\n",
    "\n",
    "OnlyAug_WT3M = [e for e in Aug_WT3M if e not in WT3M ]\n",
    "OnlyAug_WT6M = [e for e in Aug_WT6M if e not in WT6M ]\n",
    "OnlyAug_AD3M = [e for e in Aug_AD3M if e not in AD3M ]\n",
    "OnlyAug_AD6M = [e for e in Aug_AD6M if e not in AD6M ]\n",
    "\n",
    "x_WT3M = rld[WT3M,:]\n",
    "x_WT6M = rld[WT6M,:]\n",
    "x_AD3M = rld[AD3M,:]\n",
    "x_AD6M = rld[AD6M,:]\n",
    "x_OnlyAug_WT3M = rld[OnlyAug_WT3M,:]\n",
    "x_OnlyAug_WT6M = rld[OnlyAug_WT6M,:]\n",
    "x_OnlyAug_AD3M = rld[OnlyAug_AD3M,:]\n",
    "x_OnlyAug_AD6M = rld[OnlyAug_AD6M,:]\n",
    "x_real20=np.concatenate((x_WT3M,x_WT6M,x_AD3M,x_AD6M), axis=0)\n",
    "x_OnlyAug360=np.concatenate((x_OnlyAug_WT3M,x_OnlyAug_WT6M,x_OnlyAug_AD3M,x_OnlyAug_AD6M),axis=0)\n",
    "print(x_real20.shape,x_OnlyAug360.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_noise = 10000\n",
    "  \n",
    "Fake10000z = get_noise(n_noise)\n",
    "Fake10000 = generator(Fake10000z, training=False)\n",
    "print(Fake10000z.shape, Fake10000.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "n_real = 20\n",
    "fake_real_corr = np.zeros((n_real,))\n",
    "Fake20z, Fake20x = np.zeros((n_real,Fake10000z.shape[1])), np.zeros((n_real,Fake10000.shape[1]))\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    scorr_ary = np.zeros((n_noise,))\n",
    "    tmp1 = x_real20[i]\n",
    "    for j in range(n_noise):\n",
    "        tmp2 = Fake10000[j]\n",
    "        scorr_ary[j] = stats.pearsonr(tmp1, tmp2)[0]\n",
    "    smaxidx = np.argsort(scorr_ary)[-10:]\n",
    "    Fake20z[i]=np.average(tf.gather(Fake10000z, smaxidx), axis=0)\n",
    "    Fake20x[i]=generator(np.reshape(Fake20z[i],(1,100)), training=False)\n",
    "    fake_real_corr[i] = stats.pearsonr(tmp1, Fake20x[i])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(500, 300001, 500)\n",
    "Pheno_index = [WT3M,WT6M,AD3M,AD6M]\n",
    "Pheno_rld = [x_WT3M,x_WT6M,x_AD3M,x_AD6M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_training :\n",
    "    AllE_Fake20z, AllE_Fake20x, AllE_Fake20_corr = [], [], []\n",
    "    for ep in range(len(epochs)):\n",
    "        ckpath = checkpoint_dir + \"/cp-\" + str(epochs[ep]) + '.ckpt-' + str(ep+1)\n",
    "        print(\"restoring from \" + ckpath)\n",
    "        checkpoint.restore(ckpath)\n",
    "\n",
    "\n",
    "        noise_tmp = get_noise(n_noise)\n",
    "        gen_tmp = generator(noise_tmp, training=False)\n",
    "        print(noise_tmp.shape, gen_tmp.shape)\n",
    "        OneE_Fake20z, OneE_Fake20x, OneE_Fake20_corr = [], [], []\n",
    "        for pheno in range (len(Pheno_index)):\n",
    "            tmpz_rld, tmpx_rld, tmp_corr = [], [], []\n",
    "            for i in range (len(Pheno_index[pheno])):\n",
    "                scorr_arr = np.zeros((n_noise,))\n",
    "                pheno1_rld = Pheno_rld[pheno][i]\n",
    "                for j in range (n_noise):\n",
    "                    scorr_arr[j] = stats.pearsonr(pheno1_rld, gen_tmp[j])[0]\n",
    "                smaxidx = np.argsort(scorr_arr)[-10:]\n",
    "                tmpz_avg = np.average(tf.gather(noise_tmp, smaxidx), axis=0)\n",
    "                tmpx_avg = generator(np.reshape(tmpz_avg, (1, 100)), training=False)\n",
    "                tmpz_rld.append(tmpz_avg)\n",
    "                tmpx_rld.append(tmpx_avg)\n",
    "                scorr_avg = stats.pearsonr(pheno1_rld, tmpx_avg[0])[0]\n",
    "                tmp_corr.append(scorr_avg)\n",
    "\n",
    "            OneE_Fake20z.append(tmpz_rld)\n",
    "            OneE_Fake20x.append(tmpx_rld)\n",
    "            OneE_Fake20_corr.append(tmp_corr)\n",
    "        AllE_Fake20z.append(OneE_Fake20z)\n",
    "        AllE_Fake20x.append(OneE_Fake20x)\n",
    "        AllE_Fake20_corr.append(OneE_Fake20_corr)\n",
    "\n",
    "    AllE_Fake20z, AllE_Fake20x, AllE_Fake20_corr = np.array(AllE_Fake20z), np.array(AllE_Fake20x), np.array(AllE_Fake20_corr)\n",
    "    print(AllE_Fake20z.shape, AllE_Fake20x.shape, AllE_Fake20_corr.shape)\n",
    "    #save value\n",
    "    np.savez(sub_path+'/AllE_Fake20.npz', AllE_Fake20z=AllE_Fake20z,AllE_Fake20x=AllE_Fake20x,\n",
    "             AllE_Fake20_corr=AllE_Fake20_corr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reload = np.load(sub_path+'/AllE_Fake20.npz')\n",
    "ReAllE_Fake20z, ReAllE_Fake20x, ReAllE_Fake20_corr = Reload['AllE_Fake20z'],Reload['AllE_Fake20x'],Reload['AllE_Fake20_corr']\n",
    "print(ReAllE_Fake20z.shape, ReAllE_Fake20x.shape, ReAllE_Fake20_corr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suptitle=['[WT3M]','[WT6M]','[AD3M]','[AD6M]']\n",
    "plt.figure(figsize=(20,28))\n",
    "for i in range(4):\n",
    "    plt.subplot(5,1,i+1)\n",
    "    plt.subplots_adjust(hspace=0.35)\n",
    "    plt.title(suptitle[i], fontsize=20, loc='left', fontweight= 'bold')\n",
    "    plt.ylim(0.7,1.0)\n",
    "    plt.xlim(0,600)\n",
    "    plt.yticks(fontweight= 'bold',fontsize=24)\n",
    "    plt.xticks([0,50,100,150,200,250,300,350,400,450,500,550,600],\n",
    "               ['0','25K','50K','75K','100K','125K','150K','175K','200K','225K','250K','275K','300K'], \n",
    "               fontweight= 'bold',fontsize=24)\n",
    "    plt.grid(alpha=0.3)\n",
    "    for j in range(4):\n",
    "        plt.plot(ReAllE_Fake20_corr[:,i,j], linewidth=3.0)\n",
    "\n",
    "plt.subplot(5,1,5)\n",
    "plt.title('[Average]', fontsize=20, loc='left', fontweight= 'bold')\n",
    "plt.ylim(0.7,1.0)\n",
    "plt.xlim(0,600)\n",
    "plt.yticks(fontweight= 'bold',fontsize=24)\n",
    "plt.xticks([0,50,100,150,200,250,300,350,400,450,500,550,600],\n",
    "           ['0','25K','50K','75K','100K','125K','150K','175K','200K','225K','250K','275K','300K'], \n",
    "           fontweight= 'bold',fontsize=24)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.plot(np.average(np.average(ReAllE_Fake20_corr,axis=2),axis=1), linewidth=3.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolation setting \n",
    "cond_list = [Aug_WT3M,Aug_WT6M,Aug_AD3M,Aug_AD6M]\n",
    "Latent_inlist = [[cond_list[0], cond_list[1]],[cond_list[2], cond_list[3]],\n",
    "                 [cond_list[0], cond_list[2]],[cond_list[1], cond_list[3]]]\n",
    "\n",
    "\n",
    "union_condition = ['WT3MtoWT6M','AD3MtoAD6M','WT3MtoAD3M','WT6MtoAD6M']\n",
    "\n",
    "start_ep = 225000\n",
    "ep_ary = np.arange(0,50500,500)+start_ep\n",
    "start_ckpt = int(start_ep/500)\n",
    "LatInt_epoch_txt= str(start_ep)[0:3]+'K'+str(start_ep+50000)[0:3]+'K'\n",
    "print(LatInt_epoch_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolation processing\n",
    "if is_interpolation :\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    first_start = time.time()\n",
    "    #your cpu core setting\n",
    "    num_cores = (os.cpu_count()*2)-10\n",
    "\n",
    "    inlist = Latent_inlist\n",
    "    def work_func(j):\n",
    "\n",
    "        scorr_ary = stats.pearsonr(rld[s_[s]], tmp_genx_as_array[j])[0]\n",
    "        ecorr_ary = stats.pearsonr(rld[e_[s]], tmp_genx_as_array[j])[0]\n",
    "\n",
    "        return [scorr_ary,ecorr_ary]\n",
    "\n",
    "    for i in range(len(inlist)):\n",
    "        LatInt_z, LatInt_avgz, LatInt_genx  = [], [], []\n",
    "        print(\"Start \" + union_condition[i])\n",
    "\n",
    "        for ep in range(len(ep_ary)):\n",
    "            #if ep==1: break;\n",
    "            ckpath = checkpoint_dir + \"/cp-\" + str(ep_ary[ep]) + '.ckpt-' + str(ep+start_ckpt)\n",
    "            print(\"restoring from \" + ckpath)\n",
    "            checkpoint.restore(ckpath)\n",
    "\n",
    "            tmpz = get_noise(n_noise)\n",
    "            tmp_genx = generator(tmpz, training=False)\n",
    "            #Logic to avoid conflicts between TensorFlow variables and parallel processing for parallel processing\n",
    "            tmp_genx_as_array=np.array(tmp_genx,np.float32)\n",
    "            gens, gene = [], []\n",
    "\n",
    "            s_, e_ = inlist[i][0], inlist[i][1]\n",
    "\n",
    "            tmps, tmpe = [], []\n",
    "            for s in range(len(s_)):\n",
    "                #if s==1: break;\n",
    "                scorr_ary, ecorr_ary = np.zeros((n_noise,)), np.zeros((n_noise,)) \n",
    "                #parallel processing\n",
    "                if __name__ == \"__main__\":             \n",
    "\n",
    "                    pool = Pool(num_cores)\n",
    "                    work_result=np.array(pool.map(work_func,range(10000)))\n",
    "                    pool.terminate()\n",
    "\n",
    "                smaxidx = np.argsort(work_result.T[0])[-10:]\n",
    "                emaxidx = np.argsort(work_result.T[1])[-10:]\n",
    "                tmps.append(np.average(tf.gather(tmpz, smaxidx),axis=0))\n",
    "                tmpe.append(np.average(tf.gather(tmpz, emaxidx),axis=0))\n",
    "            gens.append(tmps)\n",
    "            gene.append(tmpe)\n",
    "\n",
    "            gens, gene = np.array(gens), np.array(gene)\n",
    "       \n",
    "            delta_z = np.average(gene,axis=1)-np.average(gens,axis=1)\n",
    "            inter_ = np.linspace(0., 1., num=101)\n",
    "\n",
    "            LatInt_z.append([gens, gene])\n",
    "            LatInt_avgz.append(delta_z)\n",
    "\n",
    "            #generate fake by interpolating latent vector\n",
    "            genx = []\n",
    "            for ag in range(gens.shape[0]): #age\n",
    "                tmpgenx = []\n",
    "                for aug_sp in range(gens.shape[1]): #aug_samples\n",
    "                    ttmpgenx = []\n",
    "                    #interpolate\n",
    "                    for z_ in inter_: \n",
    "                        tmpz = gens[ag][aug_sp] + (delta_z[ag]*z_)\n",
    "                        tmpx = generator(tmpz.reshape((1,100)), training=False) \n",
    "                        ttmpgenx.append(tmpx)\n",
    "                    tmpgenx.append(ttmpgenx)\n",
    "                genx.append(tmpgenx)\n",
    "            genx = np.array(genx)\n",
    "            genx = genx.reshape((len(delta_z), np.shape(gens)[1],len(inter_), p))\n",
    "\n",
    "            LatInt_genx.append(genx)\n",
    "\n",
    "        LatInt_z, LatInt_avgz = np.array(LatInt_z), np.array(LatInt_avgz)\n",
    "        LatInt_genx = np.array(LatInt_genx) \n",
    "      \n",
    "        file_name = '/LatInt_'+LatInt_epoch_txt+'_'+union_condition[i]+'.npz'\n",
    "        np.savez(sub_path+file_name, LatInt_genx=LatInt_genx) \n",
    "\n",
    "    elapsed_time = time.time() - first_start\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    print (\"Elapsed time: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genx_A, genx_S=[], []\n",
    "for i in range(len(union_condition)) :\n",
    "    #if i==1 : break\n",
    "    file_name = '/LatInt_'+LatInt_epoch_txt+'_'+union_condition[i]+'.npz'\n",
    "    Reload_LatInt = np.load(sub_path+file_name)\n",
    "    Rel_LatInt_genx = Reload_LatInt['LatInt_genx']\n",
    "    genx_A_ind = np.average(np.average(Rel_LatInt_genx, axis=2), axis=0)\n",
    "    genx_S_ind = np.std(np.average(Rel_LatInt_genx, axis=2), axis=0)\n",
    "    genx_A.append(genx_A_ind[0])\n",
    "    genx_S.append(genx_S_ind[0])\n",
    "genx_A=np.array(genx_A)\n",
    "genx_S=np.array(genx_S)\n",
    "print(genx_A.shape,genx_S.shape)\n",
    "smaple_list={'WT3M' : WT3M, 'WT6M' : WT6M, 'AD3M' : AD3M,'AD6M' : AD6M}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 4way Transition curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genx_A_reserved = np.array([np.flip(genx_A[3],axis=0),np.flip(genx_A[0],axis=0),genx_A[2],genx_A[1]])\n",
    "genx_A_sw1 = genx_A_reserved.swapaxes(1,2)\n",
    "genx_A_sw2 = genx_A_sw1.swapaxes(0,1)\n",
    "genx_A_4way = genx_A_sw2.reshape(3767,-1)\n",
    "print(union_condition)\n",
    "print(genx_A_4way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APC(Affinity Propagation Clustering)\n",
    "cluster = AffinityPropagation(random_state=0).fit(genx_A_4way)\n",
    "cluster_id = pd.DataFrame(cluster.labels_)\n",
    "cluster_id.columns = ['mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern setting\n",
    "pattern_marking_data= [[3, 10, 15, 16, 18, 22, 24, 28, 30, 32],\n",
    "                        [4, 7, 8, 9, 11, 13, 14, 17, 20, 21, 34],\n",
    "                        [2, 5, 6, 19, 23, 27],\n",
    "                        [50,51,42,45,48],\n",
    "                        [54,55],\n",
    "                        [35, 37, 41, 43, 44],\n",
    "                        [12,25,29,39],\n",
    "                        [26,33,36],\n",
    "                       [0,1,31,38,40,46,47,49,52,53]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gene mapping index data \n",
    "gene_index_data = pd.DataFrame(data=genelist,columns=['gene_symbol'])\n",
    "gene_index_data[\"gene_id\"]=np.arange(len(gene_index_data))\n",
    "gene_DKres_data = pd.read_csv('Tau_Union_res_LFC03_remove_all_case_3767genes.csv', delimiter=',')\n",
    "\n",
    "gene_DKres_data = gene_DKres_data.rename(columns={'X':'ensemble_id'})\n",
    "\n",
    "for i in range(len(gene_DKres_data)):\n",
    "    get_compare_data = gene_DKres_data.iloc[i]\n",
    "    get_comparison_value = get_compare_data['log2FoldChange']\n",
    "    get_compare_name = get_compare_data['gene_name']\n",
    "    get_mapping_id = gene_index_data[gene_index_data['gene_symbol']==get_compare_name].index  \n",
    "    \n",
    "gene_index_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marking_mapping_plot(genx_A_reshape_cluster, cluster_id, cluster_centers_indices, mapping_pattern, title,\n",
    "                         marking_pattern_fl, index):\n",
    "    \n",
    "    n_clusters_ = len(mapping_pattern)\n",
    "    s_n_clusters_= len(mapping_pattern)\n",
    "    height = n_clusters_//11  \n",
    "    if n_clusters_%11 : height=height+1\n",
    "    else : height=height+1\n",
    "    width=100 \n",
    "    if n_clusters_ < 11 :\n",
    "        height=height+1\n",
    "        s_n_clusters_=11\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(width,15))\n",
    "    print(title)\n",
    "    for i in range(s_n_clusters_):\n",
    "        \n",
    "        plt.subplot((s_n_clusters_//11)+1,11,i+1)\n",
    "        plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "        \n",
    "        #if i>n_clusters_-1 : continue\n",
    "        if n_clusters_>i :\n",
    "            num_pattern = marking_pattern_fl.index(mapping_pattern[i])+1\n",
    "            cluster_mapping_index=np.array(cluster_id[cluster_id['mapping']==mapping_pattern[i]].index)\n",
    "            plt.title('APC '+str(num_pattern)+'('+str(len(cluster_mapping_index))+')', \n",
    "                      fontsize=40, fontweight= 'bold')\n",
    "\n",
    "        \n",
    "            #print(cluster_mapping_index)\n",
    "            for j in range(len(cluster_mapping_index)):\n",
    "\n",
    "                plt.plot(genx_A_reshape_cluster[cluster_mapping_index[j]], c='gray', alpha=0.5)  \n",
    "\n",
    "                #plt.plot(genx_A[3][:,cluster_mapping_index[j]], c='gray', alpha=0.5)  \n",
    "\n",
    "            plt.plot(genx_A_reshape_cluster[cluster_centers_indices[mapping_pattern[i]]], c='red', linewidth=3)\n",
    "\n",
    "            plt.grid(True, axis='x',linewidth=3)\n",
    "            \n",
    "            plt.gca().spines['right'].set_visible(False) #오른쪽 테두리 제거\n",
    "            plt.gca().spines['top'].set_visible(False) \n",
    "            plt.gca().spines['left'].set_linewidth(4) \n",
    "            plt.gca().spines['bottom'].set_linewidth(4)\n",
    "      \n",
    "            plt.xticks([0,100,202,303,404], ('AD6M', 'WT6M','WT3M','AD3M','AD6M'),fontsize=20,\n",
    "                       fontweight= 'bold')\n",
    "            plt.yticks(fontsize=30)\n",
    "            \n",
    "            if i==0 : plt.ylabel('Rescaled RLD', fontsize=40,fontweight= 'bold')\n",
    "        else :\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.gca().spines['right'].set_visible(False) #오른쪽 테두리 제거\n",
    "            plt.gca().spines['top'].set_visible(False) \n",
    "            plt.gca().spines['left'].set_linewidth(False) \n",
    "            plt.gca().spines['bottom'].set_linewidth(False)\n",
    "    text_title = 'P'+str(index+1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_marking_data_flatten=[element for array in pattern_marking_data for element in array]\n",
    "pattern_marking_name=['P1','P2','P3','P4','P5','P6','P7','P8','Others']\n",
    "\n",
    "for i in range(len(pattern_marking_data)) :\n",
    "    marking_mapping_plot(genx_A_4way, cluster_id,\n",
    "                         cluster.cluster_centers_indices_,pattern_marking_data[i],pattern_marking_name[i],\n",
    "                         pattern_marking_data_flatten,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
